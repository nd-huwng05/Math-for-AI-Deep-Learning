# 📚 Math Roadmap for Understanding ML/DL Research Papers

This guide helps you **truly understand** the mathematical foundations behind modern ML/DL papers — not just how to solve problems, but why things work. Ideal for those who have experience solving calculus, linear algebra, and statistics problems, but want to go deeper.

---

## 🧮 1. Core Math Foundations (Essential)

### 📘 Calculus
- **Books**:
  - *Calculus* – James Stewart
  - *Thomas' Calculus* – George B. Thomas
- **What to focus on**:
  - Limits & Derivatives
  - Chain Rule, Gradients, Partial Derivatives
  - Optimization via Derivatives
  - Taylor Expansion
- **Suggested Chapters**: 2–4, 9, 14–15

---

### 📗 Linear Algebra
- **Books**:
  - *Introduction to Linear Algebra* – Gilbert Strang
  - *Linear Algebra and Its Applications* – Gilbert Strang
- **Key Topics**:
  - Vectors, Matrices, Linear Transformations
  - Eigenvalues & Eigenvectors
  - Singular Value Decomposition (SVD)
- **Suggested Chapters**: 1–3, 5–7

---

### 🎲 Probability & Statistics
- **Books**:
  - *Introduction to Probability* – Blitzstein & Hwang
  - *A First Course in Probability* – Sheldon Ross
- **Key Topics**:
  - Bayes' Theorem, Conditional Probability
  - Random Variables, Expectation
  - Distributions: Bernoulli, Binomial, Normal, etc.
  - Law of Large Numbers, Central Limit Theorem
- **Suggested Chapters**: 1–7, 12

---

## 🧠 2. Advanced Math for Research

### 📘 Mathematical Statistics
- **Books**:
  - *Statistical Inference* – Casella & Berger
  - *All of Statistics* – Larry Wasserman
- **Key Topics**:
  - Estimation (MLE, MAP), Confidence Intervals
  - Bayesian Inference, Hypothesis Testing
- **Suggested Chapters**:
  - Casella: 5–7, 10
  - Wasserman: 1–6, 8–11, 18–20

---

### 🧩 Convex Optimization
- **Book**: *Convex Optimization* – Stephen Boyd & Lieven Vandenberghe
- **Key Topics**:
  - Convex Sets & Functions
  - Duality Theory
  - Gradient-Based Optimization
- **Suggested Chapters**: 2–5

---

### 🧠 Information Theory
- **Book**: *Elements of Information Theory* – Cover & Thomas
- **Key Topics**:
  - Entropy, KL-Divergence, Mutual Information
  - Applications in VAE, GANs, RL
- **Suggested Chapters**: 2–3, 8

---

### 📐 Real Analysis (for Proofs & Convergence)
- **Books**:
  - *Understanding Analysis* – Stephen Abbott *(easier)*
  - *Principles of Mathematical Analysis* – Walter Rudin *(rigorous)*
- **Key Topics**:
  - Limits, Continuity, Sequences
  - Proof-based Thinking, Convergence
- **Suggested Chapters (Abbott)**: 1–6

---

## 🤖 3. Math for ML & Deep Learning

### 📘 Mathematics for Machine Learning *(FREE)*  
🔗 [https://mml-book.github.io/](https://mml-book.github.io/)
- **Key Topics**:
  - Linear Algebra for ML
  - Geometry, Probability, Optimization
  - Regression, PCA
- **Suggested Chapters**: 2, 3, 5–8

---

### 📗 Pattern Recognition and Machine Learning – Bishop
- **Key Topics**:
  - Probabilistic Models
  - Neural Networks
  - EM Algorithm, Mixture Models
- **Suggested Chapters**: 1–3, 5, 9

---

### 📙 Deep Learning – Goodfellow, Bengio, Courville
- **Key Topics**:
  - Linear Algebra, Probability, Optimization
  - Training Deep Nets, CNNs, Autoencoders, GANs
- **Suggested Chapters**: 2–4, 6, 8–9, 18, 20

---

## ✅ Summary Table

| Topic               | Book                          | Chapters to Read     |
|--------------------|-------------------------------|----------------------|
| Calculus           | Stewart / Thomas              | 2–4, 9, 14–15        |
| Linear Algebra     | Strang                        | 1–3, 5–7             |
| Probability        | Blitzstein / Ross             | 1–7, 12              |
| Statistics         | Casella / Wasserman           | 5–7, 10 / 1–6, 18–20 |
| Optimization       | Boyd                          | 2–5                  |
| Information Theory | Cover & Thomas                | 2–3, 8               |
| Real Analysis      | Abbott                        | 1–6                  |
| ML Foundations     | Math for ML (MML book)        | 2, 3, 5–8            |
| ML Theory          | Bishop                        | 1–3, 5, 9            |
| Deep Learning      | Goodfellow et al.             | 2–4, 6, 8–9, 18, 20  |